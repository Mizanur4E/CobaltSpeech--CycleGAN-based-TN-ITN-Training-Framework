{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9cb1387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "\n",
    "classwise_data = pd.read_csv(r'../classwiseoutputinputv3.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0eb05879",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"## Preparing train & val set\"\"\"\n",
    "#print(classwise_data)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(X_date_c)=classwise_data['X_date_c'].tolist()\n",
    "(X_letters_c)=classwise_data['X_letters_c'] [:152795].tolist()\n",
    "(X_cardinal_c)=classwise_data['X_cardinal_c'] [:133744].tolist()\n",
    "(X_verbatim_c)= classwise_data['X_verbatim_c'][:78108].tolist()\n",
    "(X_decimal_c)= classwise_data['X_decimal_c'][:9821].tolist()\n",
    "(X_measure_c)=classwise_data['X_measure_c'][:14783].tolist()\n",
    "(X_money_c)=classwise_data['X_money_c'][:6128].tolist()\n",
    "(X_ordinal_c)=classwise_data['X_ordinal_c'] [:12703].tolist()\n",
    "(X_time_c)=classwise_data['X_time_c'] [:1465].tolist()\n",
    "(X_electronic_c)=classwise_data['X_electronic_c'] [:5162].tolist()\n",
    "(X_digit_c)=classwise_data['X_digit_c'] [:5442].tolist()\n",
    "(X_fraction_c)=classwise_data['X_fraction_c'] [:1196].tolist()\n",
    "(X_telephone_c)=classwise_data['X_telephone_c'][:4024].tolist()\n",
    "(X_address_c)=classwise_data['X_address_c'] [:522].tolist()\n",
    "\n",
    "\n",
    "(X_date)=classwise_data['X_date'].tolist()\n",
    "(X_letters)=classwise_data['X_letters'] [:152795].tolist()\n",
    "(X_cardinal)=classwise_data['X_cardinal'] [:133744].tolist()\n",
    "(X_verbatim)= classwise_data['X_verbatim'][:78108].tolist()\n",
    "(X_decimal)= classwise_data['X_decimal'][:9821].tolist()\n",
    "(X_measure)=classwise_data['X_measure'][:14783].tolist()\n",
    "(X_money)=classwise_data['X_money'][:6128].tolist()\n",
    "(X_ordinal)=classwise_data['X_ordinal'] [:12703].tolist()\n",
    "(X_time)=classwise_data['X_time'] [:1465].tolist()\n",
    "(X_electronic)=classwise_data['X_electronic'] [:5162].tolist()\n",
    "(X_digit)=classwise_data['X_digit'] [:5442].tolist()\n",
    "(X_fraction)=classwise_data['X_fraction'] [:1196].tolist()\n",
    "(X_telephone)=classwise_data['X_telephone'][:4024].tolist()\n",
    "(X_address)=classwise_data['X_address'] [:522].tolist()\n",
    "\n",
    "\n",
    "\n",
    "(y_date)=classwise_data['y_date'].tolist() \n",
    "(y_letters)=classwise_data['y_letters'] [:152795].tolist()\n",
    "(y_cardinal)=classwise_data['y_cardinal']  [:133744].tolist()\n",
    "(y_verbatim)=classwise_data['y_verbatim'][:78108].tolist()\n",
    "(y_decimal)=classwise_data['y_decimal'][:9821].tolist()\n",
    "(y_measure)=classwise_data['y_measure'][:14783].tolist()\n",
    "(y_money)=classwise_data['y_money'] [:6128].tolist()\n",
    "(y_ordinal)=classwise_data['y_ordinal']  [:12703].tolist()\n",
    "(y_time)=classwise_data['y_time'] [:1465].tolist()\n",
    "(y_electronic)=classwise_data['y_electronic']  [:5162].tolist()\n",
    "(y_digit)=classwise_data['y_digit']  [:5442].tolist()\n",
    "(y_fraction)=classwise_data['y_fraction'][:1196].tolist()\n",
    "(y_telephone)=classwise_data['y_telephone'] [:4024].tolist()\n",
    "(y_address)=classwise_data['y_address'] [:522].tolist()\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train_date_c, X_val_date_c, X_train_date, X_val_date, y_train_date, y_val_date = train_test_split(X_date_c,X_date, y_date, test_size=0.015, random_state=42)\n",
    "X_train_letters_c, X_val_letters_c, X_train_letters, X_val_letters, y_train_letters, y_val_letters = train_test_split(X_letters_c,X_letters, y_letters, test_size=0.015, random_state=42)\n",
    "X_train_cardinal_c, X_val_cardinal_c,X_train_cardinal, X_val_cardinal, y_train_cardinal, y_val_cardinal = train_test_split(X_cardinal_c,X_cardinal, y_cardinal, test_size=0.015, random_state=42)\n",
    "X_train_verbatim_c, X_val_verbatim_c,X_train_verbatim, X_val_verbatim, y_train_verbatim, y_val_verbatim = train_test_split(X_verbatim_c,X_verbatim, y_verbatim, test_size=0.015, random_state=42)\n",
    "X_train_decimal_c, X_val_decimal_c,X_train_decimal, X_val_decimal, y_train_decimal, y_val_decimal = train_test_split(X_decimal_c,X_decimal, y_decimal, test_size=0.015, random_state=42)\n",
    "X_train_measure_c, X_val_measure_c,X_train_measure, X_val_measure, y_train_measure, y_val_measure = train_test_split(X_measure_c,X_measure, y_measure, test_size=0.015, random_state=42)\n",
    "X_train_money_c, X_val_money_c,X_train_money, X_val_money, y_train_money, y_val_money = train_test_split(X_money_c,X_money, y_money, test_size=0.015, random_state=42)\n",
    "X_train_ordinal_c, X_val_ordinal_c,X_train_ordinal, X_val_ordinal, y_train_ordinal, y_val_ordinal = train_test_split(X_ordinal_c, X_ordinal, y_ordinal, test_size=0.015, random_state=42)\n",
    "X_train_time_c, X_val_time_c,X_train_time, X_val_time, y_train_time, y_val_time = train_test_split(X_time_c, X_time, y_time, test_size=0.015, random_state=42)\n",
    "X_train_electronic_c, X_val_electronic_c,X_train_electronic, X_val_electronic, y_train_electronic, y_val_electronic = train_test_split(X_electronic_c, X_electronic, y_electronic, test_size=0.015, random_state=42)\n",
    "X_train_digit_c, X_val_digit_c,X_train_digit, X_val_digit, y_train_digit, y_val_digit = train_test_split(X_digit_c,X_digit, y_digit, test_size=0.015, random_state=42)\n",
    "X_train_fraction_c, X_val_fraction_c, X_train_fraction, X_val_fraction, y_train_fraction, y_val_fraction = train_test_split(X_fraction_c, X_fraction, y_fraction, test_size=0.015, random_state=42)\n",
    "X_train_telephone_c, X_val_telephone_c,X_train_telephone, X_val_telephone, y_train_telephone, y_val_telephone = train_test_split(X_telephone_c,X_telephone, y_telephone, test_size=0.015, random_state=42)\n",
    "X_train_address_c, X_val_address_c,X_train_address, X_val_address, y_train_address, y_val_address = train_test_split(X_address_c, X_address, y_address, test_size=0.015, random_state=42)\n",
    "\n",
    "\n",
    "X_train_c =X_train_date_c+X_train_letters_c+X_train_cardinal_c+X_train_verbatim_c+X_train_decimal_c+X_train_measure_c+X_train_money_c+X_train_ordinal_c+X_train_time_c+X_train_electronic_c+X_train_digit_c+ X_train_fraction_c+X_train_telephone_c+X_train_address_c\n",
    "X_train =X_train_date+X_train_letters+X_train_cardinal+X_train_verbatim+X_train_decimal+X_train_measure+X_train_money+X_train_ordinal+X_train_time+X_train_electronic+X_train_digit+ X_train_fraction+X_train_telephone+X_train_address\n",
    "y_train= y_train_date+y_train_letters+y_train_cardinal+y_train_verbatim+y_train_decimal+y_train_measure+y_train_money+y_train_ordinal+y_train_time+y_train_electronic+y_train_digit+y_train_fraction+y_train_telephone+y_train_address\n",
    "\n",
    "\n",
    "X_val_c =X_val_date_c+X_val_letters_c+X_val_cardinal_c+X_val_verbatim_c+X_val_decimal_c+X_val_measure_c+X_val_money_c+X_val_ordinal_c+X_val_time_c+X_val_electronic_c+X_val_digit_c+ X_val_fraction_c+X_val_telephone_c+X_val_address_c\n",
    "X_val =X_val_date+X_val_letters+X_val_cardinal+X_val_verbatim+X_val_decimal+X_val_measure+X_val_money+X_val_ordinal+X_val_time+X_val_electronic+X_val_digit+ X_val_fraction+X_val_telephone+X_val_address\n",
    "y_val= y_val_date+y_val_letters+y_val_cardinal+y_val_verbatim+y_val_decimal+y_val_measure+y_val_money+y_val_ordinal+y_val_time+y_val_electronic+y_val_digit+y_val_fraction+y_val_telephone+y_val_address\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "782ea3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['362 ad  ', '2011 ', '2011 ', 'february 1  2007 ', '1910 ', '18 june 1946 ', '1400 ', '2009 ', '1997 ', '19 march 1875 ']\n",
      "['362 ad', '2011', '2011', 'february 1  2007', '1910', '18 june 1946', '1400', '2009', '1997', '19 march 1875']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X_val[:10])\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i] = X_train[i].strip()\n",
    "    y_train[i] = y_train[i].strip()\n",
    "    X_train_c[i]= X_train_c[i].strip()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    X_val[i] = X_val[i].strip()\n",
    "    y_val[i] = y_val[i].strip()\n",
    "    X_val_c[i]= X_val_c[i].strip()\n",
    "print(X_val[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46169ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1011\n",
      "['releases bbc', 'the 1990s', 'science 2832']\n",
      "['bbc', '1990s', '2832']\n",
      "['b b c', 'nineteen nineties', 'two thousand eight hundred thirty two']\n",
      "999 999 999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train_c,X_test_c,X_train,X_test,y_train,y_test = train_test_split( X_train_c,X_train, y_train,test_size=0.0015)\n",
    "\n",
    "print(len(X_test_c))\n",
    "datasetLength = 1000 #max data size 673000\n",
    "\n",
    "X_nnrmlzd_c= X_train_c[0:datasetLength]\n",
    "X_nnrmlzd= X_train[0:datasetLength]\n",
    "X_nrmlzd = y_train[0:datasetLength]\n",
    "\n",
    "\n",
    "\n",
    "X_train_c,_,X_train,_,y_train, _ = train_test_split( X_nnrmlzd_c,X_nnrmlzd, X_nrmlzd,test_size=0.0000000001) #shuffles all data\n",
    "\n",
    "X_nnrmlzd_c=X_train_c \n",
    "X_nnrmlzd =X_train\n",
    "X_nrmlzd =y_train\n",
    "\n",
    "print(X_nnrmlzd_c[1:4])\n",
    "print(X_nnrmlzd[1:4])\n",
    "print(X_nrmlzd[1:4])\n",
    "print(len(X_nnrmlzd_c),len(X_nnrmlzd),len(X_nrmlzd))\n",
    "\n",
    "pad_size=1\n",
    "maxlen1 = 8\n",
    "maxlen2 = 12  #10 represents number of word generated from one not normalized word\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34fe6571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3  29  58 270   0   0   0   0]\n",
      " [271 109   0   0   0   0   0   0]\n",
      " [  4 172   0   0   0   0   0   0]\n",
      " [173 272   0   0   0   0   0   0]\n",
      " [  1 273   0   0   0   0   0   0]]\n",
      "[[ 29  58 270   0   0   0   0   0]\n",
      " [109   0   0   0   0   0   0   0]\n",
      " [172   0   0   0   0   0   0   0]\n",
      " [272   0   0   0   0   0   0   0]\n",
      " [273   0   0   0   0   0   0   0]]\n",
      "[[ 14 101  17  61  40  30   7   0   0   0   0   0]\n",
      " [ 37  37  13   0   0   0   0   0   0   0   0   0]\n",
      " [  3 111   0   0   0   0   0   0   0   0   0   0]\n",
      " [  1   4  10   8  38   1   0   0   0   0   0   0]\n",
      " [ 13  33  11  11   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"### Tokenize text and padding\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "tokenizer1 = Tokenizer(num_words=datasetLength )\n",
    "tokenizer2 = Tokenizer(num_words=datasetLength )\n",
    "tokenizer1.fit_on_texts(X_nnrmlzd_c)\n",
    "tokenizer2.fit_on_texts(X_nrmlzd)\n",
    "\n",
    "\n",
    "X_nnrmlzd_c = tokenizer1.texts_to_sequences(X_nnrmlzd_c)\n",
    "X_nnrmlzd = tokenizer1.texts_to_sequences(X_nnrmlzd)\n",
    "X_nrmlzd =  tokenizer2.texts_to_sequences(X_nrmlzd)\n",
    "\n",
    "vocab_size1 = len(tokenizer1.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "vocab_size2 = len(tokenizer2.word_index) + 1\n",
    "\n",
    "X_nnrmlzd_c = pad_sequences(X_nnrmlzd_c, padding='post', maxlen=maxlen1)\n",
    "X_nnrmlzd = pad_sequences(X_nnrmlzd, padding='post', maxlen=maxlen1)\n",
    "X_nrmlzd = pad_sequences(X_nrmlzd, padding='post', maxlen=maxlen2)\n",
    "\n",
    "\n",
    "\n",
    "#print(vocab_size1,vocab_size2)\n",
    "\n",
    "#print(tokenizer1.index_word)\n",
    "\n",
    "print(X_nnrmlzd_c[:5])\n",
    "print(X_nnrmlzd[:5])\n",
    "print(X_nrmlzd[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "668fe431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check keras version\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "if keras.__version__ < '2.3.1':\n",
    "\tprint('Use Keras 2.3.1 or later')\n",
    "\texit(1)\n",
    "# If multiple copies of the OpenMP runtime is available,\n",
    "# setting 'KMP_DUPLICATE_LIB_OK=TRUE' allows the program to execute\n",
    "# I am facing this issue after I have upgraded to Tensorflow 2.\n",
    "from os import environ\n",
    "environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense,Lambda\n",
    "from keras import backend as K\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten, Maximum\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.models import Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from random import random\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import asarray\n",
    "from numpy import argmax\n",
    "from numpy import array\n",
    "from numpy.random import randint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from numpy.random import shuffle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e4df358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scoring Fucntions\n",
    "#functions for generating BLEU, ACCURACY and WER \n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from jiwer import wer \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def average_bleu(refa, hypa):\n",
    "    \n",
    "  smoothie = SmoothingFunction().method2\n",
    "  n= len(refa)\n",
    "  net =0\n",
    "  for i in range(n):\n",
    "    \n",
    "    a= sentence_bleu([refa[i]],hypa[i], smoothing_function=smoothie)\n",
    "    net= net + a\n",
    "  me = net/n\n",
    "  return me\n",
    "\n",
    "def acc_scorer(y_pred,y_true):\n",
    "    a= 0\n",
    "    for i in range(len(y_true)):\n",
    "        a+= accuracy_score(y_true[i],y_pred[i])\n",
    "    a= a/len(y_true)\n",
    "    return a\n",
    "def accuracy_printerXY(X,y_true,gen):\n",
    "    inp= tokenizer1.texts_to_sequences(X)\n",
    "    inp =pad_sequences(inp, padding='post', maxlen=maxlen1)\n",
    "    out = gen.predict(inp)\n",
    "    out = np.argmax(out,axis=-1)\n",
    "    text_out= tokenizer2.sequences_to_texts(out)\n",
    "    ground_truth =y_true\n",
    "\n",
    "    ground_truth_seq = tokenizer2.texts_to_sequences(ground_truth)\n",
    "    ground_truth_seq =pad_sequences(ground_truth_seq, padding='post', maxlen=maxlen2)\n",
    "    text_out= np.asarray(text_out)\n",
    "    ground_truth= ground_truth\n",
    "    text_out= text_out.tolist()\n",
    "    \n",
    "    acc= acc_scorer(out,ground_truth_seq)\n",
    "\n",
    "    print(text_out[:10])\n",
    "    print('------------------------------')\n",
    "    print(ground_truth[:10])\n",
    "    w= wer(ground_truth,text_out)\n",
    "    print('WER is:', w*100)\n",
    "\n",
    "    print('Accuracy is:',acc)\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    score = average_bleu(ground_truth,text_out)\n",
    "    print('Average BLEU score is: ',score)\n",
    "    print('\\n')\n",
    "    \n",
    "    return w,acc,score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9210721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"## Functions for generating training data samples\"\"\"\n",
    "\n",
    "\"all set to go\"\n",
    "def generate_real_samples2(dataset_c,dataset, n_samples, out_shape):\n",
    "\t# choose random instances\n",
    "\tix = randint(0, dataset.shape[0], n_samples)\n",
    "\t# extract selected sequences\n",
    "\tX_c = dataset_c[ix]\n",
    "\tX = dataset[ix]\n",
    "\t# generate 'real' class labels (1)\n",
    "\ty = ones((n_samples, out_shape))\n",
    "\treturn X_c,X, y\n",
    "\n",
    "# select a batch of random samples, returns real dequences and target labels\n",
    "def generate_real_samples(dataset, n_samples, out_shape):\n",
    "\t# choose random instances\n",
    "\tix = randint(0, dataset.shape[0], n_samples)\n",
    "\t# extract selected sequences\n",
    "\tX = dataset[ix]\n",
    "\t# generate 'real' class labels (1)\n",
    "\ty = ones((n_samples, out_shape))\n",
    "\treturn X, y\n",
    "\n",
    "# generate a batch of fake sequences (produced by the generators) and taget labels\n",
    "def generate_fake_samples(g_model, dataset, out_shape):\n",
    "\t# generate fake instance\n",
    "\tX_sftmax = g_model.predict(dataset)\n",
    "\tX_int = list()\n",
    "\tfor mat in X_sftmax:\n",
    "\t\tvec = [argmax(i) for i in mat]\n",
    "\t\tX_int.append(vec)\n",
    "\tX = array(X_int)\n",
    "\t# create 'fake' class labels (0)\n",
    "\ty = zeros((len(X), out_shape))\n",
    "\treturn X, y\n",
    "\n",
    "\n",
    "# update data pool for fake sequences\n",
    "def update_data_pool(pool, data, max_size=500):\n",
    "\tselected = list()\n",
    "\tfor seq in data:\n",
    "\t\tif len(pool) < max_size:\n",
    "\t\t\t# stock the pool\n",
    "\t\t\tpool.append(seq)\n",
    "\t\t\tselected.append(seq)\n",
    "\t\telif random() < 0.5:\n",
    "\t\t\t# use data, but don't add it to the pool\n",
    "\t\t\tselected.append(seq)\n",
    "\t\telse:\n",
    "\t\t\t# replace an existing seq and use replaced seq\n",
    "\t\t\tix = randint(0, len(pool))\n",
    "\t\t\tselected.append(pool[ix])\n",
    "\t\t\tpool[ix] = seq\n",
    "\treturn asarray(selected)\n",
    "\n",
    "# select a batch of random samples for seq2seq training\n",
    "def generate_real_samples_seq(trainX_c,trainX, trainY, n_samples):\n",
    "\t# choose random instances\n",
    "\tix = randint(0, trainX.shape[0], n_samples)\n",
    "\t# retrieve selected images\n",
    "\tX_c = trainX_c[ix]\n",
    "\tX = trainX[ix]\n",
    "\tY = trainY[ix]\n",
    "\treturn X_c,X, Y\n",
    "\n",
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "\tylist = list()\n",
    "\tfor sequence in sequences:\n",
    "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "\t\tylist.append(encoded)\n",
    "\ty = array(ylist)\n",
    "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "\treturn y\n",
    "\n",
    "\"all set to go\"\n",
    "\n",
    "'''\n",
    "# define generator model\n",
    "def define_generator(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "\tmodel.add(LSTM(n_units))\n",
    "\tmodel.add(RepeatVector(tar_timesteps))\n",
    "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "\t# compile model\n",
    "\tmodel.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\t# summarize defined model\n",
    "\n",
    "\treturn model\n",
    "'''\n",
    "\n",
    "\n",
    "\"all set to go\"\n",
    "\n",
    "\n",
    "# define generator model\n",
    "def define_generator(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "\tmodel.add(Bidirectional(LSTM(n_units)))\n",
    "\tmodel.add(RepeatVector(tar_timesteps))\n",
    "\tmodel.add(Bidirectional(LSTM(n_units, return_sequences=True)))\n",
    "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "\t# compile model\n",
    "\tmodel.compile(optimizer='adam', loss='categorical_crossentropy',metrics= ['accuracy'], run_eagerly=False)\n",
    "\t# summarize defined model\n",
    "\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# define discriminator model\n",
    "def define_discriminator(vocab_size, max_length):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "\tmodel.add(Conv1D(filters=32, kernel_size=2, activation='relu'))\n",
    "\tmodel.add(MaxPooling1D(pool_size=2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(10, activation='relu'))\n",
    "\tmodel.add(Dense(1, activation='sigmoid'))\n",
    "\t# compile network\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# summarize defined model\n",
    "\n",
    "\treturn model\n",
    "\n",
    "\"all set to go\"\n",
    "\n",
    "# define a composite model for updating generators by adversarial and cycle loss\n",
    "def define_composite_model(g_model_1, d_model, g_model_2, timestep1, timestep2):\n",
    "\t# ensure the model we're updating is trainable\n",
    "\tg_model_1.trainable = True\n",
    "\t# mark discriminator as not trainable\n",
    "\td_model.trainable = False\n",
    "\t# mark other generator model as not trainable\n",
    "\tg_model_2.trainable = False\n",
    "\t# discriminator element\n",
    "\tinput1 = Input(shape=(timestep1,))\n",
    "\tgen1_out_sftmax = g_model_1(input1)\n",
    "\tArgmaxLayer = Lambda(lambda x: K.cast(K.argmax(x), dtype='float32'))\n",
    "\tgen1_out = ArgmaxLayer(gen1_out_sftmax)\n",
    "\toutput_d = d_model(gen1_out)\n",
    "\t# forward cycle\n",
    "\toutput_f = g_model_2(gen1_out)\n",
    "\t# backward cycle\n",
    "\tinput2 = Input(shape=(timestep2,))\n",
    "\tgen2_out_sftmax = g_model_2(input2)\n",
    "\tgen2_out = ArgmaxLayer(gen2_out_sftmax)\n",
    "\toutput_b= g_model_1(gen2_out)\n",
    "\t# define model graph\n",
    "\tmodel = Model([input1, input2], [output_d, output_f, output_b])\n",
    "\t# define optimization algorithm configuration\n",
    "\topt = Adam(lr=0.0002, beta_1=0.2)\n",
    "\t# compile model with weighting of least squares loss and L1 loss\n",
    "\tmodel.compile(loss=['binary_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'], loss_weights=[1, 5.0, 5.0], optimizer=opt)\n",
    "\n",
    "\treturn model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "888fa793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 12) for input KerasTensor(type_spec=TensorSpec(shape=(None, 12), dtype=tf.float32, name='embedding_5_input'), name='embedding_5_input', description=\"created by layer 'embedding_5_input'\"), but it was called on an input with incompatible shape (32, 1).\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "2 root error(s) found.\n  (0) Unimplemented:  Cast string to float is not supported\n\t [[node sequential_6/Cast (defined at <ipython-input-34-4e62014815b6>:27) ]]\n  (1) Cancelled:  Function was cancelled before it was started\n0 successful operations.\n0 derived errors ignored. [Op:__inference_predict_function_89617]\n\nFunction call stack:\npredict_function -> predict_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-4838ced3e6db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mc_model_YtoX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_composite_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_model_YtoX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_model_XtoY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestepY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestepX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# train models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_model_XtoY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_model_YtoX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_model_XtoY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_model_YtoX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\"\"\"--------------------------------Let's finish it---------------------------------\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-4838ced3e6db>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(d_model_X, d_model_Y, g_model_XtoY, g_model_YtoX, c_model_XtoY, c_model_YtoX, dataset, vocabSize, tokenizer)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mX_real_trainX_c\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_real_trainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_real_trainY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_real_samples_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX_c\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# generate a batch of fake samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mX_fakeX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fakeX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_fake_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_model_YtoX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_realY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mX_fakeY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fakeY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_fake_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_model_XtoY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_realX_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_out\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m#think again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# update fakes from pool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-4e62014815b6>\u001b[0m in \u001b[0;36mgenerate_fake_samples\u001b[0;34m(g_model, dataset, out_shape)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_fake_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# generate fake instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mX_sftmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mX_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_sftmax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1749\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1751\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1752\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1753\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    955\u001b[0m       \u001b[0;31m# If we did not create any variables the trace we have is good enough.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m       return self._concrete_stateful_fn._call_flat(\n\u001b[0;32m--> 957\u001b[0;31m           filtered_flat_args, self._concrete_stateful_fn.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfn_with_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_kwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_filtered_flat_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: 2 root error(s) found.\n  (0) Unimplemented:  Cast string to float is not supported\n\t [[node sequential_6/Cast (defined at <ipython-input-34-4e62014815b6>:27) ]]\n  (1) Cancelled:  Function was cancelled before it was started\n0 successful operations.\n0 derived errors ignored. [Op:__inference_predict_function_89617]\n\nFunction call stack:\npredict_function -> predict_function\n"
     ]
    }
   ],
   "source": [
    "\"\"\"## Function for training cyclegan models\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(d_model_X, d_model_Y, g_model_XtoY, g_model_YtoX, c_model_XtoY, c_model_YtoX, dataset, vocabSize, tokenizer):\n",
    "    # define properties of the training run\n",
    "    n_epochs, n_batch, = 8, 128\n",
    "    # determine the output shape of the discriminator\n",
    "    n_out = d_model_X.output_shape[1]\n",
    "    # unpack dataset and train-test split\n",
    "    trainX_c, trainX, trainY = dataset\n",
    "    \n",
    "\n",
    "    vocabSizeX, vocabSizeY = vocabSize\n",
    "    tokenizerX ,tokenizerY = tokenizer\n",
    "    # prepare data pool for fakes\n",
    "    poolX, poolY = list(), list()\n",
    "    # calculate the number of batches per training epoch\n",
    "    bat_per_epo = int(len(trainX) / n_batch)\n",
    "    # calculate the number of training iterations\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    # manually enumerate epochs\n",
    "    gX_loss = list()\n",
    "    gY_loss = list()\n",
    "    \n",
    "    for i in range(n_steps):\n",
    "        # select a batch of real samples\n",
    "        X_realX_c,X_realX, y_realX = generate_real_samples2(trainX_c,trainX, n_batch, n_out)\n",
    "        X_realY, y_realY = generate_real_samples(trainY, n_batch, n_out)\n",
    "        X_real_trainX_c,X_real_trainX, X_real_trainY = generate_real_samples_seq(trainX_c,trainX, trainY, n_batch)\n",
    "        # generate a batch of fake samples\n",
    "        X_fakeX, y_fakeX = generate_fake_samples(g_model_YtoX, X_realY, n_out)\n",
    "        X_fakeY, y_fakeY = generate_fake_samples(g_model_XtoY, X_realX_c, n_out)    #think again\n",
    "        # update fakes from pool\n",
    "        X_fakeX = update_data_pool(poolX, X_fakeX)\n",
    "        X_fakeY = update_data_pool(poolY, X_fakeY)\n",
    "        # convert integer output to softmax\n",
    "        X_real_trainX_sftmax = encode_output(X_real_trainX, vocabSizeX)\n",
    "        X_real_trainY_sftmax = encode_output(X_real_trainY, vocabSizeY)\n",
    "        X_realX_sftmax = encode_output(X_realX, vocabSizeX)\n",
    "        X_realY_sftmax = encode_output(X_realY, vocabSizeY)\n",
    "        # update generator Y->X via seq2seq loss\n",
    "        gy_loss = g_model_YtoX.train_on_batch(X_real_trainY, X_real_trainX_sftmax)  \n",
    "        # update generator Y->X via adversarial and cycle loss\n",
    "        g_loss2, _, _, _  = c_model_YtoX.train_on_batch([X_realY, X_realX_c], [y_realX, X_realY_sftmax, X_realX_sftmax]) #see here\n",
    "        # update discriminator for X -> [real/fake]\n",
    "        dX_loss1, _ = d_model_X.train_on_batch(X_realX, y_realX)\n",
    "        dX_loss2, _ = d_model_X.train_on_batch(X_fakeX, y_fakeX)\n",
    "        # update generator X->Y via seq2seq loss\n",
    "        gx_loss = g_model_XtoY.train_on_batch(X_real_trainX_c, X_real_trainY_sftmax)\n",
    "        # update generator X->Y via adversarial and cycle loss\n",
    "        g_loss1, _, _, _  = c_model_XtoY.train_on_batch([X_realX_c, X_realY], [y_realY, X_realX_sftmax, X_realY_sftmax])\n",
    "        # update discriminator for Y -> [real/fake]\n",
    "        dY_loss1, _ = d_model_Y.train_on_batch(X_realY, y_realY)\n",
    "        dY_loss2, _ = d_model_Y.train_on_batch(X_fakeY, y_fakeY)\n",
    "        # summarize performance\n",
    "        print('>%d, dX[%.3f,%.3f] dY[%.3f,%.3f] g[%.3f,%.3f] gX[%.3f] gY[%.3f]' % (i+1, dX_loss1,dX_loss2, dY_loss1,dY_loss2, g_loss1,g_loss2,gx_loss,gy_loss))\n",
    "        # evaluate the model performance every so often\n",
    "        if (i+1) % (bat_per_epo * 1) == 0:\n",
    "            accuracy_printerXY(X_val_c,y_val,g_model_XtoY)\n",
    "            accuracy_printerXY(X_val,y_val,g_model_XtoY)\n",
    "            gX_loss.append(gx_loss)\n",
    "            gY_loss.append(gy_loss)\n",
    "            #g_model_XtoY.save('modelXY_V2WV')\n",
    "            #g_model_YtoX.save('modelYX_V2WV')\n",
    "            \n",
    "    print(gX_loss)\n",
    "    print(gY_loss)\n",
    "\n",
    "tokenizer =[tokenizer1,tokenizer2]\n",
    "timestepX = maxlen1\n",
    "timestepY = maxlen2\n",
    "\n",
    "\"All set to go !!\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\t# load dataset\n",
    "\n",
    "\tdataX_c = np.array(X_train_c)\n",
    "\tdataX = np.array(X_train)\n",
    "\tdataY =  np.array(y_train)\n",
    "  \n",
    "\n",
    "\t\n",
    "\tdataset = [dataX_c,dataX, dataY]\n",
    "\n",
    "\tvocabSize =[vocab_size1,vocab_size2]\n",
    "\t\n",
    "\t\n",
    "\t# generator: X -> Y\n",
    "\tg_model_XtoY = define_generator(vocabSize[0], vocabSize[1], timestepX, timestepY, 256)\n",
    "\t# generator: Y -> X\n",
    "\tg_model_YtoX = define_generator(vocabSize[1], vocabSize[0], timestepY, timestepX, 256)\n",
    "\t# discriminator: X -> [spoken/written]\n",
    "\td_model_X = define_discriminator(vocabSize[0], timestepX)\n",
    "\t# discriminator: B -> [spoken/written]\n",
    "\td_model_Y = define_discriminator(vocabSize[1], timestepY)\n",
    "\t# composite: X -> Y -> [spoken/written, X]\n",
    "\tc_model_XtoY = define_composite_model(g_model_XtoY, d_model_Y, g_model_YtoX, timestepX, timestepY)\n",
    "\t# composite: Y -> X -> [spoken/written, Y]\n",
    "\tc_model_YtoX = define_composite_model(g_model_YtoX, d_model_X, g_model_XtoY, timestepY, timestepX)\n",
    "\t# train models\n",
    "\ttrain(d_model_X, d_model_Y, g_model_XtoY, g_model_YtoX, c_model_XtoY, c_model_YtoX, dataset, vocabSize, tokenizer)\n",
    "\n",
    "\"\"\"--------------------------------Let's finish it---------------------------------\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca697c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_c= np.array(X_train_c)\n",
    "print(X_train_c.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

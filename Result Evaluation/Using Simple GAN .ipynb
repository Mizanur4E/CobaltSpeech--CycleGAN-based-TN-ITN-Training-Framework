{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "JOvzVqTJQFF4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nayan/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,15,16,17,18,19,20,21,22,23,24,25,26,27,29,30,31,32,33,34,35,36,37,38,39,40,41) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "\n",
    "classwise_data = pd.read_csv(r'../classwiseoutputinputv3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "K1JJTFvnQbRw"
   },
   "outputs": [],
   "source": [
    "\"\"\"## Preparing train & val set\"\"\"\n",
    "#print(classwise_data)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(X_date_c)=classwise_data['X_date_c'].tolist()\n",
    "(X_letters_c)=classwise_data['X_letters_c'] [:152795].tolist()\n",
    "(X_cardinal_c)=classwise_data['X_cardinal_c'] [:133744].tolist()\n",
    "(X_verbatim_c)= classwise_data['X_verbatim_c'][:78108].tolist()\n",
    "(X_decimal_c)= classwise_data['X_decimal_c'][:9821].tolist()\n",
    "(X_measure_c)=classwise_data['X_measure_c'][:14783].tolist()\n",
    "(X_money_c)=classwise_data['X_money_c'][:6128].tolist()\n",
    "(X_ordinal_c)=classwise_data['X_ordinal_c'] [:12703].tolist()\n",
    "(X_time_c)=classwise_data['X_time_c'] [:1465].tolist()\n",
    "(X_electronic_c)=classwise_data['X_electronic_c'] [:5162].tolist()\n",
    "(X_digit_c)=classwise_data['X_digit_c'] [:5442].tolist()\n",
    "(X_fraction_c)=classwise_data['X_fraction_c'] [:1196].tolist()\n",
    "(X_telephone_c)=classwise_data['X_telephone_c'][:4024].tolist()\n",
    "(X_address_c)=classwise_data['X_address_c'] [:522].tolist()\n",
    "\n",
    "\n",
    "(X_date)=classwise_data['X_date'].tolist()\n",
    "(X_letters)=classwise_data['X_letters'] [:152795].tolist()\n",
    "(X_cardinal)=classwise_data['X_cardinal'] [:133744].tolist()\n",
    "(X_verbatim)= classwise_data['X_verbatim'][:78108].tolist()\n",
    "(X_decimal)= classwise_data['X_decimal'][:9821].tolist()\n",
    "(X_measure)=classwise_data['X_measure'][:14783].tolist()\n",
    "(X_money)=classwise_data['X_money'][:6128].tolist()\n",
    "(X_ordinal)=classwise_data['X_ordinal'] [:12703].tolist()\n",
    "(X_time)=classwise_data['X_time'] [:1465].tolist()\n",
    "(X_electronic)=classwise_data['X_electronic'] [:5162].tolist()\n",
    "(X_digit)=classwise_data['X_digit'] [:5442].tolist()\n",
    "(X_fraction)=classwise_data['X_fraction'] [:1196].tolist()\n",
    "(X_telephone)=classwise_data['X_telephone'][:4024].tolist()\n",
    "(X_address)=classwise_data['X_address'] [:522].tolist()\n",
    "\n",
    "\n",
    "\n",
    "(y_date)=classwise_data['y_date'].tolist() \n",
    "(y_letters)=classwise_data['y_letters'] [:152795].tolist()\n",
    "(y_cardinal)=classwise_data['y_cardinal']  [:133744].tolist()\n",
    "(y_verbatim)=classwise_data['y_verbatim'][:78108].tolist()\n",
    "(y_decimal)=classwise_data['y_decimal'][:9821].tolist()\n",
    "(y_measure)=classwise_data['y_measure'][:14783].tolist()\n",
    "(y_money)=classwise_data['y_money'] [:6128].tolist()\n",
    "(y_ordinal)=classwise_data['y_ordinal']  [:12703].tolist()\n",
    "(y_time)=classwise_data['y_time'] [:1465].tolist()\n",
    "(y_electronic)=classwise_data['y_electronic']  [:5162].tolist()\n",
    "(y_digit)=classwise_data['y_digit']  [:5442].tolist()\n",
    "(y_fraction)=classwise_data['y_fraction'][:1196].tolist()\n",
    "(y_telephone)=classwise_data['y_telephone'] [:4024].tolist()\n",
    "(y_address)=classwise_data['y_address'] [:522].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "yp1EzcH_pRaF"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train_date_c, X_val_date_c, X_train_date, X_val_date, y_train_date, y_val_date = train_test_split(X_date_c,X_date, y_date, test_size=0.015, random_state=42)\n",
    "X_train_letters_c, X_val_letters_c, X_train_letters, X_val_letters, y_train_letters, y_val_letters = train_test_split(X_letters_c,X_letters, y_letters, test_size=0.015, random_state=42)\n",
    "X_train_cardinal_c, X_val_cardinal_c,X_train_cardinal, X_val_cardinal, y_train_cardinal, y_val_cardinal = train_test_split(X_cardinal_c,X_cardinal, y_cardinal, test_size=0.015, random_state=42)\n",
    "X_train_verbatim_c, X_val_verbatim_c,X_train_verbatim, X_val_verbatim, y_train_verbatim, y_val_verbatim = train_test_split(X_verbatim_c,X_verbatim, y_verbatim, test_size=0.015, random_state=42)\n",
    "X_train_decimal_c, X_val_decimal_c,X_train_decimal, X_val_decimal, y_train_decimal, y_val_decimal = train_test_split(X_decimal_c,X_decimal, y_decimal, test_size=0.015, random_state=42)\n",
    "X_train_measure_c, X_val_measure_c,X_train_measure, X_val_measure, y_train_measure, y_val_measure = train_test_split(X_measure_c,X_measure, y_measure, test_size=0.015, random_state=42)\n",
    "X_train_money_c, X_val_money_c,X_train_money, X_val_money, y_train_money, y_val_money = train_test_split(X_money_c,X_money, y_money, test_size=0.015, random_state=42)\n",
    "X_train_ordinal_c, X_val_ordinal_c,X_train_ordinal, X_val_ordinal, y_train_ordinal, y_val_ordinal = train_test_split(X_ordinal_c, X_ordinal, y_ordinal, test_size=0.015, random_state=42)\n",
    "X_train_time_c, X_val_time_c,X_train_time, X_val_time, y_train_time, y_val_time = train_test_split(X_time_c, X_time, y_time, test_size=0.015, random_state=42)\n",
    "X_train_electronic_c, X_val_electronic_c,X_train_electronic, X_val_electronic, y_train_electronic, y_val_electronic = train_test_split(X_electronic_c, X_electronic, y_electronic, test_size=0.015, random_state=42)\n",
    "X_train_digit_c, X_val_digit_c,X_train_digit, X_val_digit, y_train_digit, y_val_digit = train_test_split(X_digit_c,X_digit, y_digit, test_size=0.015, random_state=42)\n",
    "X_train_fraction_c, X_val_fraction_c, X_train_fraction, X_val_fraction, y_train_fraction, y_val_fraction = train_test_split(X_fraction_c, X_fraction, y_fraction, test_size=0.015, random_state=42)\n",
    "X_train_telephone_c, X_val_telephone_c,X_train_telephone, X_val_telephone, y_train_telephone, y_val_telephone = train_test_split(X_telephone_c,X_telephone, y_telephone, test_size=0.015, random_state=42)\n",
    "X_train_address_c, X_val_address_c,X_train_address, X_val_address, y_train_address, y_val_address = train_test_split(X_address_c, X_address, y_address, test_size=0.015, random_state=42)\n",
    "\n",
    "\n",
    "X_train_c =X_train_date_c+X_train_letters_c+X_train_cardinal_c+X_train_verbatim_c+X_train_decimal_c+X_train_measure_c+X_train_money_c+X_train_ordinal_c+X_train_time_c+X_train_electronic_c+X_train_digit_c+ X_train_fraction_c+X_train_telephone_c+X_train_address_c\n",
    "X_train =X_train_date+X_train_letters+X_train_cardinal+X_train_verbatim+X_train_decimal+X_train_measure+X_train_money+X_train_ordinal+X_train_time+X_train_electronic+X_train_digit+ X_train_fraction+X_train_telephone+X_train_address\n",
    "y_train= y_train_date+y_train_letters+y_train_cardinal+y_train_verbatim+y_train_decimal+y_train_measure+y_train_money+y_train_ordinal+y_train_time+y_train_electronic+y_train_digit+y_train_fraction+y_train_telephone+y_train_address\n",
    "\n",
    "\n",
    "X_val_c =X_val_date_c+X_val_letters_c+X_val_cardinal_c+X_val_verbatim_c+X_val_decimal_c+X_val_measure_c+X_val_money_c+X_val_ordinal_c+X_val_time_c+X_val_electronic_c+X_val_digit_c+ X_val_fraction_c+X_val_telephone_c+X_val_address_c\n",
    "X_val =X_val_date+X_val_letters+X_val_cardinal+X_val_verbatim+X_val_decimal+X_val_measure+X_val_money+X_val_ordinal+X_val_time+X_val_electronic+X_val_digit+ X_val_fraction+X_val_telephone+X_val_address\n",
    "y_val= y_val_date+y_val_letters+y_val_cardinal+y_val_verbatim+y_val_decimal+y_val_measure+y_val_money+y_val_ordinal+y_val_time+y_val_electronic+y_val_digit+y_val_fraction+y_val_telephone+y_val_address\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "AsB1oJFOQ9-Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['362 ad  ', '2011 ', '2011 ', 'february 1  2007 ', '1910 ', '18 june 1946 ', '1400 ', '2009 ', '1997 ', '19 march 1875 ']\n",
      "['362 ad', '2011', '2011', 'february 1  2007', '1910', '18 june 1946', '1400', '2009', '1997', '19 march 1875']\n"
     ]
    }
   ],
   "source": [
    "print(X_val[:10])\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i] = X_train[i].strip()\n",
    "    y_train[i] = y_train[i].strip()\n",
    "    X_train_c[i]= X_train_c[i].strip()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    X_val[i] = X_val[i].strip()\n",
    "    y_val[i] = y_val[i].strip()\n",
    "    X_val_c[i]= X_val_c[i].strip()\n",
    "print(X_val[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "cB6c7KXbRW5v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1011\n",
      "['from 1997 to', 'in 23 super', 'о г ю']\n",
      "['1997', '23', 'г']\n",
      "['nineteen ninety seven', 'twenty three', 'г']\n",
      "399999 399999 399999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train_c,X_test_c,X_train,X_test,y_train,y_test = train_test_split( X_train_c,X_train, y_train,test_size=0.0015)\n",
    "\n",
    "print(len(X_test_c))\n",
    "datasetLength = 400000 #max data size 673000\n",
    "\n",
    "X_nnrmlzd_c= X_train_c[0:datasetLength]\n",
    "X_nnrmlzd= X_train[0:datasetLength]\n",
    "X_nrmlzd = y_train[0:datasetLength]\n",
    "\n",
    "\n",
    "\n",
    "X_train_c,_,X_train,_,y_train, _ = train_test_split( X_nnrmlzd_c,X_nnrmlzd, X_nrmlzd,test_size=0.0000000001) #shuffles all data\n",
    "\n",
    "X_nnrmlzd_c=X_train_c \n",
    "X_nnrmlzd =X_train\n",
    "X_nrmlzd =y_train\n",
    "\n",
    "print(X_nnrmlzd_c[1:4])\n",
    "print(X_nnrmlzd[1:4])\n",
    "print(X_nrmlzd[1:4])\n",
    "print(len(X_nnrmlzd_c),len(X_nnrmlzd),len(X_nrmlzd))\n",
    "\n",
    "pad_size=1\n",
    "maxlen1 = 6\n",
    "maxlen2 = 10  #10 represents number of word generated from one not normalized word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWizFcBFRbRl"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"### Tokenize text and padding\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "tokenizer1 = Tokenizer(num_words=datasetLength )\n",
    "tokenizer2 = Tokenizer(num_words=datasetLength )\n",
    "tokenizer1.fit_on_texts(X_nnrmlzd_c)\n",
    "tokenizer2.fit_on_texts(X_nrmlzd)\n",
    "\n",
    "\n",
    "X_nnrmlzd = tokenizer1.texts_to_sequences(X_nnrmlzd_c)\n",
    "X_nrmlzd =  tokenizer2.texts_to_sequences(X_nrmlzd)\n",
    "\n",
    "vocab_size1 = len(tokenizer1.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "vocab_size2 = len(tokenizer2.word_index) + 1\n",
    "\n",
    "\n",
    "X_nnrmlzd = pad_sequences(X_nnrmlzd, padding='post', maxlen=maxlen1)\n",
    "X_nrmlzd = pad_sequences(X_nrmlzd, padding='post', maxlen=maxlen2)\n",
    "\n",
    "print(vocab_size1,vocab_size2)\n",
    "\n",
    "print(X_nnrmlzd[:5])\n",
    "print(X_nrmlzd[:5])\n",
    "\n",
    "X_train=(X_nnrmlzd)\n",
    "y_train=(X_nrmlzd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0_2QQOJGRltX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jEtcjjoJR6I5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aC3vyhh3MQzn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ck3tBmF4QYIP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rF49TE--SDlj"
   },
   "outputs": [],
   "source": [
    "# Check keras version\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "if keras.__version__ < '2.3.1':\n",
    "\tprint('Use Keras 2.3.1 or later')\n",
    "\texit(1)\n",
    "# If multiple copies of the OpenMP runtime is available,\n",
    "# setting 'KMP_DUPLICATE_LIB_OK=TRUE' allows the program to execute\n",
    "# I am facing this issue after I have upgraded to Tensorflow 2.\n",
    "from os import environ\n",
    "environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense,Lambda\n",
    "from keras import backend as K\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten, Maximum\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.models import Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from random import random\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import asarray\n",
    "from numpy import argmax\n",
    "from numpy import array\n",
    "from numpy.random import randint\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from numpy.random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PNe0em2ar84D"
   },
   "outputs": [],
   "source": [
    "#tf.config.run_functions_eagerly(True)       #solves a value error related to config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJffdeNsLntR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYXhj04UpW0A"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"## Data processing for discriminator Training\"\"\"\n",
    "\n",
    "# select normalized samples\n",
    "def generate_Xreal_Y_samples(dataset, n_samples):\n",
    "    # choose random instances\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    # select not normalized words\n",
    "    X = dataset[ix]\n",
    "    # create inverted labels for the fake samples\n",
    "    y = ones((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_Xfake_Y_samples(generator,dataset, n_samples):\n",
    "    # choose random instances\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    # select not normalized words\n",
    "    x_input  = dataset[ix]\n",
    "    # predict outputs\n",
    "    X = generator.predict(x_input)\n",
    "    # create class labels\n",
    "    y = zeros((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "\"\"\"## Data Processing for generator training\"\"\"\n",
    "\n",
    "# select notnormalized samples\n",
    "def generate_notnormalized_samplesGAN(dataset, n_samples):\n",
    "    # choose random instances\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    # select not normalized words\n",
    "    X = dataset[ix]\n",
    "    # create inverted labels for the fake samples\n",
    "    y = ones((n_samples, 1))\n",
    "    return X, y\n",
    "\n",
    "\n",
    "    # save the generator model\n",
    "    #g_model.save('/content/drive/My Drive/text normalization/data/generator.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7g6ZbGFSonY"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"## Define Discriminator, Generator and GAN model\"\"\"\n",
    "\n",
    "# define discriminator model\n",
    "\n",
    "# define discriminator model\n",
    "def define_discriminator(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    #model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Conv1D(filters=32, kernel_size=2, activation='relu' ,input_shape=(max_length,vocab_size)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# define generator model\n",
    "def define_generator(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "\tmodel.add(Bidirectional(LSTM(n_units)))\n",
    "\tmodel.add(RepeatVector(tar_timesteps))\n",
    "\tmodel.add(Bidirectional(LSTM(n_units, return_sequences=True)))\n",
    "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "\t# compile model\n",
    "\tmodel.compile(optimizer='adam', loss='categorical_crossentropy',metrics= ['accuracy'], run_eagerly=False)\n",
    "\t# summarize defined model\n",
    "\n",
    "\treturn model\n",
    " \n",
    "'''\n",
    "# define generator model\n",
    "def define_generator(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "  model.add(LSTM(n_units))\n",
    "  model.add(RepeatVector(tar_timesteps))\n",
    "  model.add(LSTM(n_units, return_sequences=True))\n",
    "  model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    " \n",
    "  # compile model\n",
    "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "  # summarize defined model\n",
    "  model.summary()\n",
    "  return model\n",
    " \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    " \n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "  # make weights in the discriminator not trainable\n",
    "  d_model.trainable = False\n",
    "  g_model.trainable= True\n",
    "  # connect them\n",
    "  model = Sequential()\n",
    "  model.add(g_model)\n",
    "  #ArgmaxLayer = Lambda(lambda x: K.argmax(x,axis=-1))\n",
    "  #model.add(ArgmaxLayer)\n",
    "  #model.add(Dense(1,activation='relu'))\n",
    "  model.add(d_model)\n",
    "  # compile model\n",
    "  opt = Adam(lr=0.0001, beta_1=0.5)\n",
    "  model.compile(loss=['binary_crossentropy'], optimizer=opt,metrics=['accuracy'])\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hvic2mQRTEuh"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"## GAN training\"\"\"\n",
    "\n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, dataX, dataY, n_epochs=13, n_batch=128):\n",
    "    bat_per_epo = int(dataX.shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "\n",
    "        # enumerate batches over the training set\n",
    "        for j in range(bat_per_epo):\n",
    "\n",
    "            # get randomly selected 'real' samples\n",
    "            X_real, y_real = generate_Xreal_Y_samples(dataY, half_batch)\n",
    "            X_real = to_categorical(X_real,num_classes=vocab_size2)\n",
    "            # update discriminator model weights\n",
    "            d_loss1, _ = d_model.train_on_batch(X_real, y_real)\n",
    "            # generate 'fake' examples\n",
    "            X_fake, y_fake = generate_Xfake_Y_samples(g_model,dataX, half_batch)\n",
    "            #X_fake = np.argmax(X_fake,axis=-1)\n",
    "            # update discriminator model weights\n",
    "            d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\n",
    "            #print (d_loss1)\n",
    "            ix = randint(0, dataX.shape[0], n_batch)\n",
    "            y_t = to_categorical(dataY[ix],num_classes = vocab_size2)\n",
    "            g_loss_self, _ = g_model.train_on_batch(dataX[ix], y_t)\n",
    "            # prepare input output for gan model\n",
    "            X_gan, y_gan =  generate_notnormalized_samplesGAN(dataX, n_batch)\n",
    "            \n",
    "            #print('hey', X_gan.shape,y_gan.shape)\n",
    "            # update the generator via the discriminator's error\n",
    "            g_loss,_ = gan_model.train_on_batch(X_gan,y_gan)\n",
    "            d_loss = (d_loss1+d_loss2)/2\n",
    "            # summarize loss on this batch\n",
    "           \n",
    "            if j % 15 == 0:\n",
    "              print('->epoch[%d],batch_no[%d],gX_self[%.5f], gX[%.5f], dX[%.5f]'% (i+1,j+1,g_loss_self,g_loss,d_loss)) \n",
    "        accuracy_printerXY(X_val_c,y_val,g_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctqvJKXAwAv3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# create the discriminator\n",
    "discriminator = define_discriminator(vocab_size2,maxlen2)\n",
    "# create the generator\n",
    "generator = define_generator(vocab_size1,vocab_size2,maxlen1,maxlen2,128)\n",
    "# create the gan\n",
    "gan_model = define_gan(generator, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= generator\n",
    "\n",
    "#Scoring Fucntions\n",
    "#functions for generating BLEU, ACCURACY and WER \n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from jiwer import wer \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def average_bleu(refa, hypa):\n",
    "    \n",
    "  smoothie = SmoothingFunction().method2\n",
    "  n= len(refa)\n",
    "  net =0\n",
    "  for i in range(n):\n",
    "    \n",
    "    a= sentence_bleu([refa[i]],hypa[i], smoothing_function=smoothie)\n",
    "    net= net + a\n",
    "  me = net/n\n",
    "  return me\n",
    "\n",
    "def acc_scorer(y_pred,y_true):\n",
    "    a= 0\n",
    "    for i in range(len(y_true)):\n",
    "        a+= accuracy_score(y_true[i],y_pred[i])\n",
    "    a= a/len(y_true)\n",
    "    return a\n",
    "def accuracy_printerXY(X,y_true,gen):\n",
    "    inp= tokenizer1.texts_to_sequences(X)\n",
    "    inp =pad_sequences(inp, padding='post', maxlen=maxlen1)\n",
    "    out = gen.predict(inp)\n",
    "    out = np.argmax(out,axis=-1)\n",
    "    text_out= tokenizer2.sequences_to_texts(out)\n",
    "    ground_truth =y_true\n",
    "\n",
    "    ground_truth_seq = tokenizer2.texts_to_sequences(ground_truth)\n",
    "    ground_truth_seq =pad_sequences(ground_truth_seq, padding='post', maxlen=maxlen2)\n",
    "    text_out= np.asarray(text_out)\n",
    "    ground_truth= ground_truth\n",
    "    text_out= text_out.tolist()\n",
    "    \n",
    "    acc= acc_scorer(out,ground_truth_seq)\n",
    "\n",
    "    print(text_out[:10])\n",
    "    print('------------------------------')\n",
    "    print(ground_truth[:10])\n",
    "    w= wer(ground_truth,text_out)\n",
    "    print('WER is:', w*100)\n",
    "\n",
    "    print('Accuracy is:',acc)\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    score = average_bleu(ground_truth,text_out)\n",
    "    print('Average BLEU score is: ',score)\n",
    "    print('\\n')\n",
    "    \n",
    "    return w,acc,score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qVSxyZoKQ6D7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(generator, discriminator, gan_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1CMgI8Mse6m"
   },
   "source": [
    "### Inferencing and Scoring classwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = [3876 , 2292 , 2007 , 1172 , 148 , 222 , 92 , 191 , 22 , 78 , 82 , 18 , 61 , 8]\n",
    "classes =['date','letters','cardinal','verbatim','decimal','measure','money','ordinal','time',\n",
    "          'electronic','digit', 'fraction','telephone','address']\n",
    "\n",
    "terminal=0\n",
    "initial =0\n",
    "\n",
    "wer_classwise = []\n",
    "acc_classwise = []\n",
    "bleu_classwise = []\n",
    "for i in range(14):\n",
    "    \n",
    "    terminal= initial + edges[i]\n",
    "    print('For class: ',classes[i])\n",
    "    wr, acc, bleu = accuracy_printerXY(X_val_c[initial:terminal],y_val[initial:terminal],generator)\n",
    "    wer_classwise.append(wr)\n",
    "    acc_classwise.append(acc)\n",
    "    bleu_classwise.append(bleu)\n",
    "    \n",
    "    initial = terminal\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "print('WER classwise:',wer_classwise)\n",
    "print('Accuracy classwise:',acc_classwise)\n",
    "print('BLEU classwise', bleu_classwise)\n",
    "\n",
    "\n",
    "accuracy_printerXY(X_val_c,y_val,generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#inferencing sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGdP8Q4ocsHj",
    "outputId": "b1302cbc-20ce-4d72-e834-ebf4c644b341"
   },
   "outputs": [],
   "source": [
    "a= ['3rd person','make 1','1st day',' 90 kg','2 cm','10 cm','10 km','5 m','34 man','12 person','8 pm','june 9 2012 day']\n",
    "\n",
    "a_s= tokenizer1.texts_to_sequences(a)\n",
    "k=a_s\n",
    "ot= []\n",
    "for a in k:\n",
    "  a= generator.predict([a])\n",
    "  a= np.argmax(a,axis= -1)\n",
    "  out= tokenizer2.sequences_to_texts(a)\n",
    "  ot.append(out)\n",
    "print(ot)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Gan final v2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c33b083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4487aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import gc\n",
    "import os \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edb15ff7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nayan/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (1,2,3,4,5,6,7,8,9,10,11,12,13,15,16,17,18,19,20,21,22,23,24,25,26,27,29,30,31,32,33,34,35,36,37,38,39,40,41) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "classwise_data = pd.read_csv(r'../classwiseoutputinputv3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b6992b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## Preparing train & val set\"\"\"\n",
    "#print(classwise_data)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(X_date_c)=classwise_data['X_date_c'].tolist()\n",
    "(X_letters_c)=classwise_data['X_letters_c'] [:152795].tolist()\n",
    "(X_cardinal_c)=classwise_data['X_cardinal_c'] [:133744].tolist()\n",
    "(X_verbatim_c)= classwise_data['X_verbatim_c'][:78108].tolist()\n",
    "(X_decimal_c)= classwise_data['X_decimal_c'][:9821].tolist()\n",
    "(X_measure_c)=classwise_data['X_measure_c'][:14783].tolist()\n",
    "(X_money_c)=classwise_data['X_money_c'][:6128].tolist()\n",
    "(X_ordinal_c)=classwise_data['X_ordinal_c'] [:12703].tolist()\n",
    "(X_time_c)=classwise_data['X_time_c'] [:1465].tolist()\n",
    "(X_electronic_c)=classwise_data['X_electronic_c'] [:5162].tolist()\n",
    "(X_digit_c)=classwise_data['X_digit_c'] [:5442].tolist()\n",
    "(X_fraction_c)=classwise_data['X_fraction_c'] [:1196].tolist()\n",
    "(X_telephone_c)=classwise_data['X_telephone_c'][:4024].tolist()\n",
    "(X_address_c)=classwise_data['X_address_c'] [:522].tolist()\n",
    "\n",
    "\n",
    "(X_date)=classwise_data['X_date'].tolist()\n",
    "(X_letters)=classwise_data['X_letters'] [:152795].tolist()\n",
    "(X_cardinal)=classwise_data['X_cardinal'] [:133744].tolist()\n",
    "(X_verbatim)= classwise_data['X_verbatim'][:78108].tolist()\n",
    "(X_decimal)= classwise_data['X_decimal'][:9821].tolist()\n",
    "(X_measure)=classwise_data['X_measure'][:14783].tolist()\n",
    "(X_money)=classwise_data['X_money'][:6128].tolist()\n",
    "(X_ordinal)=classwise_data['X_ordinal'] [:12703].tolist()\n",
    "(X_time)=classwise_data['X_time'] [:1465].tolist()\n",
    "(X_electronic)=classwise_data['X_electronic'] [:5162].tolist()\n",
    "(X_digit)=classwise_data['X_digit'] [:5442].tolist()\n",
    "(X_fraction)=classwise_data['X_fraction'] [:1196].tolist()\n",
    "(X_telephone)=classwise_data['X_telephone'][:4024].tolist()\n",
    "(X_address)=classwise_data['X_address'] [:522].tolist()\n",
    "\n",
    "\n",
    "\n",
    "(y_date)=classwise_data['y_date'].tolist() \n",
    "(y_letters)=classwise_data['y_letters'] [:152795].tolist()\n",
    "(y_cardinal)=classwise_data['y_cardinal']  [:133744].tolist()\n",
    "(y_verbatim)=classwise_data['y_verbatim'][:78108].tolist()\n",
    "(y_decimal)=classwise_data['y_decimal'][:9821].tolist()\n",
    "(y_measure)=classwise_data['y_measure'][:14783].tolist()\n",
    "(y_money)=classwise_data['y_money'] [:6128].tolist()\n",
    "(y_ordinal)=classwise_data['y_ordinal']  [:12703].tolist()\n",
    "(y_time)=classwise_data['y_time'] [:1465].tolist()\n",
    "(y_electronic)=classwise_data['y_electronic']  [:5162].tolist()\n",
    "(y_digit)=classwise_data['y_digit']  [:5442].tolist()\n",
    "(y_fraction)=classwise_data['y_fraction'][:1196].tolist()\n",
    "(y_telephone)=classwise_data['y_telephone'] [:4024].tolist()\n",
    "(y_address)=classwise_data['y_address'] [:522].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18768711",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train_date_c, X_val_date_c, X_train_date, X_val_date, y_train_date, y_val_date = train_test_split(X_date_c,X_date, y_date, test_size=0.015, random_state=42)\n",
    "X_train_letters_c, X_val_letters_c, X_train_letters, X_val_letters, y_train_letters, y_val_letters = train_test_split(X_letters_c,X_letters, y_letters, test_size=0.015, random_state=42)\n",
    "X_train_cardinal_c, X_val_cardinal_c,X_train_cardinal, X_val_cardinal, y_train_cardinal, y_val_cardinal = train_test_split(X_cardinal_c,X_cardinal, y_cardinal, test_size=0.015, random_state=42)\n",
    "X_train_verbatim_c, X_val_verbatim_c,X_train_verbatim, X_val_verbatim, y_train_verbatim, y_val_verbatim = train_test_split(X_verbatim_c,X_verbatim, y_verbatim, test_size=0.015, random_state=42)\n",
    "X_train_decimal_c, X_val_decimal_c,X_train_decimal, X_val_decimal, y_train_decimal, y_val_decimal = train_test_split(X_decimal_c,X_decimal, y_decimal, test_size=0.015, random_state=42)\n",
    "X_train_measure_c, X_val_measure_c,X_train_measure, X_val_measure, y_train_measure, y_val_measure = train_test_split(X_measure_c,X_measure, y_measure, test_size=0.015, random_state=42)\n",
    "X_train_money_c, X_val_money_c,X_train_money, X_val_money, y_train_money, y_val_money = train_test_split(X_money_c,X_money, y_money, test_size=0.015, random_state=42)\n",
    "X_train_ordinal_c, X_val_ordinal_c,X_train_ordinal, X_val_ordinal, y_train_ordinal, y_val_ordinal = train_test_split(X_ordinal_c, X_ordinal, y_ordinal, test_size=0.015, random_state=42)\n",
    "X_train_time_c, X_val_time_c,X_train_time, X_val_time, y_train_time, y_val_time = train_test_split(X_time_c, X_time, y_time, test_size=0.015, random_state=42)\n",
    "X_train_electronic_c, X_val_electronic_c,X_train_electronic, X_val_electronic, y_train_electronic, y_val_electronic = train_test_split(X_electronic_c, X_electronic, y_electronic, test_size=0.015, random_state=42)\n",
    "X_train_digit_c, X_val_digit_c,X_train_digit, X_val_digit, y_train_digit, y_val_digit = train_test_split(X_digit_c,X_digit, y_digit, test_size=0.015, random_state=42)\n",
    "X_train_fraction_c, X_val_fraction_c, X_train_fraction, X_val_fraction, y_train_fraction, y_val_fraction = train_test_split(X_fraction_c, X_fraction, y_fraction, test_size=0.015, random_state=42)\n",
    "X_train_telephone_c, X_val_telephone_c,X_train_telephone, X_val_telephone, y_train_telephone, y_val_telephone = train_test_split(X_telephone_c,X_telephone, y_telephone, test_size=0.015, random_state=42)\n",
    "X_train_address_c, X_val_address_c,X_train_address, X_val_address, y_train_address, y_val_address = train_test_split(X_address_c, X_address, y_address, test_size=0.015, random_state=42)\n",
    "\n",
    "\n",
    "X_train_c =X_train_date_c+X_train_letters_c+X_train_cardinal_c+X_train_verbatim_c+X_train_decimal_c+X_train_measure_c+X_train_money_c+X_train_ordinal_c+X_train_time_c+X_train_electronic_c+X_train_digit_c+ X_train_fraction_c+X_train_telephone_c+X_train_address_c\n",
    "X_train =X_train_date+X_train_letters+X_train_cardinal+X_train_verbatim+X_train_decimal+X_train_measure+X_train_money+X_train_ordinal+X_train_time+X_train_electronic+X_train_digit+ X_train_fraction+X_train_telephone+X_train_address\n",
    "y_train= y_train_date+y_train_letters+y_train_cardinal+y_train_verbatim+y_train_decimal+y_train_measure+y_train_money+y_train_ordinal+y_train_time+y_train_electronic+y_train_digit+y_train_fraction+y_train_telephone+y_train_address\n",
    "\n",
    "\n",
    "X_val_c =X_val_date_c+X_val_letters_c+X_val_cardinal_c+X_val_verbatim_c+X_val_decimal_c+X_val_measure_c+X_val_money_c+X_val_ordinal_c+X_val_time_c+X_val_electronic_c+X_val_digit_c+ X_val_fraction_c+X_val_telephone_c+X_val_address_c\n",
    "X_val =X_val_date+X_val_letters+X_val_cardinal+X_val_verbatim+X_val_decimal+X_val_measure+X_val_money+X_val_ordinal+X_val_time+X_val_electronic+X_val_digit+ X_val_fraction+X_val_telephone+X_val_address\n",
    "y_val= y_val_date+y_val_letters+y_val_cardinal+y_val_verbatim+y_val_decimal+y_val_measure+y_val_money+y_val_ordinal+y_val_time+y_val_electronic+y_val_digit+y_val_fraction+y_val_telephone+y_val_address\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87170626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['362 ad  ', '2011 ', '2011 ', 'february 1  2007 ', '1910 ', '18 june 1946 ', '1400 ', '2009 ', '1997 ', '19 march 1875 ']\n",
      "['362 ad', '2011', '2011', 'february 1  2007', '1910', '18 june 1946', '1400', '2009', '1997', '19 march 1875']\n"
     ]
    }
   ],
   "source": [
    "print(X_val[:10])\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i] = X_train[i].strip()\n",
    "    y_train[i] = y_train[i].strip()\n",
    "    X_train_c[i]= X_train_c[i].strip()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    X_val[i] = X_val[i].strip()\n",
    "    y_val[i] = y_val[i].strip()\n",
    "    X_val_c[i]= X_val_c[i].strip()\n",
    "print(X_val[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c7b7f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = [X_val_c, X_val,y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b95ccd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1011\n",
      "['of 8 061', '1797', '2015 10 12']\n",
      "['8 061', '1797', '2015 10 12']\n",
      "['eight thousand sixty one', 'seventeen ninety seven', 'the twelfth of october twenty fifteen']\n",
      "9999 9999 9999\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train_c,X_test_c,X_train,X_test,y_train,y_test = train_test_split( X_train_c,X_train, y_train,test_size=0.0015)\n",
    "\n",
    "print(len(X_test_c))\n",
    "datasetLength = 10000 #max data size 673000\n",
    "\n",
    "X_nnrmlzd_c= X_train_c[0:datasetLength]\n",
    "X_nnrmlzd= X_train[0:datasetLength]\n",
    "X_nrmlzd = y_train[0:datasetLength]\n",
    "\n",
    "\n",
    "\n",
    "X_train_c,_,X_train,_,y_train, _ = train_test_split( X_nnrmlzd_c,X_nnrmlzd, X_nrmlzd,test_size=0.0000000001) #shuffles all data\n",
    "\n",
    "X_nnrmlzd_c=X_train_c \n",
    "X_nnrmlzd =X_train\n",
    "X_nrmlzd =y_train\n",
    "\n",
    "print(X_nnrmlzd_c[1:4])\n",
    "print(X_nnrmlzd[1:4])\n",
    "print(X_nrmlzd[1:4])\n",
    "print(len(X_nnrmlzd_c),len(X_nnrmlzd),len(X_nrmlzd))\n",
    "\n",
    "pad_size=1\n",
    "maxlen1 = 6\n",
    "maxlen2 = 10  #10 represents number of word generated from one not normalized word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c00b1af",
   "metadata": {},
   "source": [
    "### change after this for changing model, all before is to preparing and importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dc71bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5699 411\n",
      "[[   3  257  716    0    0    0]\n",
      " [   6   42 1531    0    0    0]\n",
      " [1532    0    0    0    0    0]\n",
      " [  31   28   55    0    0    0]\n",
      " [  95    0    0    0    0    0]]\n",
      "[[34 34 19  0  0  0  0  0  0  0]\n",
      " [13  4 37  5  0  0  0  0  0  0]\n",
      " [78 21 14  0  0  0  0  0  0  0]\n",
      " [ 6 88  7 53  2 47  0  0  0  0]\n",
      " [ 1  4  1  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"### Tokenize text and padding\"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "tokenizer1 = Tokenizer(num_words=datasetLength )\n",
    "tokenizer2 = Tokenizer(num_words=datasetLength )\n",
    "tokenizer1.fit_on_texts(X_nnrmlzd_c)\n",
    "tokenizer2.fit_on_texts(X_nrmlzd)\n",
    "\n",
    "\n",
    "X_nnrmlzd = tokenizer1.texts_to_sequences(X_nnrmlzd_c)\n",
    "X_nrmlzd =  tokenizer2.texts_to_sequences(X_nrmlzd)\n",
    "\n",
    "vocab_size1 = len(tokenizer1.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "vocab_size2 = len(tokenizer2.word_index) + 1\n",
    "\n",
    "\n",
    "X_nnrmlzd = pad_sequences(X_nnrmlzd, padding='post', maxlen=maxlen1)\n",
    "X_nrmlzd = pad_sequences(X_nrmlzd, padding='post', maxlen=maxlen2)\n",
    "\n",
    "print(vocab_size1,vocab_size2)\n",
    "\n",
    "print(X_nnrmlzd[:5])\n",
    "print(X_nrmlzd[:5])\n",
    "\n",
    "X_train=(X_nnrmlzd)\n",
    "y_train=(X_nrmlzd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c213286",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y_train, num_classes= vocab_size2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e09e70c",
   "metadata": {},
   "source": [
    "### define model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f798689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check keras version\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except:\n",
    "        # Invalid device or cannot modify virtual devices once initialized.\n",
    "        pass\n",
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense,Lambda\n",
    "from keras import backend as K\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from numpy.random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d403221",
   "metadata": {},
   "source": [
    "### Seq2seq model encoder-decoder based with bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17ac87ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# define generator model\\ndef define_generator(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\\n\\tmodel = Sequential()\\n\\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\\n\\tmodel.add(Bidirectional(LSTM(n_units)))\\n\\tmodel.add(RepeatVector(tar_timesteps))\\n\\tmodel.add(Bidirectional(LSTM(n_units, return_sequences=True)))\\n\\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\\n\\t# compile model\\n\\tmodel.compile(optimizer='adam', loss='categorical_crossentropy',metrics= ['accuracy'], run_eagerly=False)\\n\\t# summarize defined model\\n\\n\\treturn model\\n\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# define generator model\n",
    "def define_generator(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "\tmodel.add(Bidirectional(LSTM(n_units)))\n",
    "\tmodel.add(RepeatVector(tar_timesteps))\n",
    "\tmodel.add(Bidirectional(LSTM(n_units, return_sequences=True)))\n",
    "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "\t# compile model\n",
    "\tmodel.compile(optimizer='adam', loss='categorical_crossentropy',metrics= ['accuracy'], run_eagerly=False)\n",
    "\t# summarize defined model\n",
    "\n",
    "\treturn model\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c192fb32",
   "metadata": {},
   "source": [
    "### Adding attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e724f90",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Recurrent'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2242644e744c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_time_distributed_dense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Recurrent'"
     ]
    }
   ],
   "source": [
    " from keras.layers.recurrent import Recurrent, _time_distributed_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2b1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention_decoder import AttentionDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b7f3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead00f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(150, input_shape=(n_timesteps_in, n_features), return_sequences=True))\n",
    "    model.add(AttentionDecoder(150, n_features))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    '''\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(150, input_shape=(n_timesteps_in, n_features)))\n",
    "    model.add(RepeatVector(n_timesteps_in))\n",
    "    model.add(LSTM(150, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(n_features, activation='softmax')))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])'''\n",
    "\n",
    "    model = Sequential()\n",
    "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "\tmodel.add(Bidirectional(LSTM(n_units, return_sequences=True)))\n",
    "    model.add(AttentionDecoder(n_units, tar_vocab))\n",
    "    \n",
    "    '''\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "\tmodel.add(Bidirectional(LSTM(n_units, return_sequences=True)))\n",
    "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    '''\n",
    "\t\n",
    "\t# compile model\n",
    "\tmodel.compile(optimizer='adam', loss='categorical_crossentropy',metrics= ['accuracy'], run_eagerly=False)\n",
    "\t# summarize defined model\n",
    "\n",
    "\t\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9121c2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5cf664",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_generator(vocab_size1,vocab_size2,maxlen1,maxlen2,256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0db5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb827ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train,y, batch_size = 128, epochs = 5, verbose = 2,validation_split= 0.0015,shuffle= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef752367",
   "metadata": {},
   "source": [
    "Inferencing and scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2bb79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from jiwer import wer \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def average_bleu(refa, hypa):\n",
    "    \n",
    "  smoothie = SmoothingFunction().method2\n",
    "  n= len(refa)\n",
    "  net =0\n",
    "  for i in range(n):\n",
    "    \n",
    "    a= sentence_bleu([refa[i]],hypa[i], smoothing_function=smoothie)\n",
    "    net= net + a\n",
    "  me = net/n\n",
    "  return me\n",
    "\n",
    "def acc_scorer(y_pred,y_true):\n",
    "    a= 0\n",
    "    for i in range(len(y_true)):\n",
    "        a+= accuracy_score(y_true[i],y_pred[i])\n",
    "    a= a/len(y_true)\n",
    "    return a\n",
    "def accuracy_printerXY(X,y_true,gen):\n",
    "    inp= tokenizer1.texts_to_sequences(X)\n",
    "    inp =pad_sequences(inp, padding='post', maxlen=maxlen1)\n",
    "    out = gen.predict(inp)\n",
    "    out = np.argmax(out,axis=-1)\n",
    "    text_out= tokenizer2.sequences_to_texts(out)\n",
    "    ground_truth =y_true\n",
    "\n",
    "    ground_truth_seq = tokenizer2.texts_to_sequences(ground_truth)\n",
    "    ground_truth_seq =pad_sequences(ground_truth_seq, padding='post', maxlen=maxlen2)\n",
    "    text_out= np.asarray(text_out)\n",
    "    ground_truth= ground_truth\n",
    "    text_out= text_out.tolist()\n",
    "    \n",
    "    acc= acc_scorer(out,ground_truth_seq)\n",
    "\n",
    "    print(text_out[:10])\n",
    "    print('------------------------------')\n",
    "    print(ground_truth[:10])\n",
    "    w= wer(ground_truth,text_out)\n",
    "    print('WER is:', w*100)\n",
    "\n",
    "    print('Accuracy is:',acc)\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    score = average_bleu(ground_truth,text_out)\n",
    "    print('Average BLEU score is: ',score)\n",
    "    print('\\n')\n",
    "    \n",
    "    return w,acc,score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a697a844",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = [3876 , 2292 , 2007 , 1172 , 148 , 222 , 92 , 191 , 22 , 78 , 82 , 18 , 61 , 8]\n",
    "classes =['date','letters','cardinal','verbatim','decimal','measure','money','ordinal','time',\n",
    "          'electronic','digit', 'fraction','telephone','address']\n",
    "\n",
    "terminal=0\n",
    "initial =0\n",
    "\n",
    "wer_classwise = []\n",
    "acc_classwise = []\n",
    "bleu_classwise = []\n",
    "for i in range(14):\n",
    "    \n",
    "    terminal= initial + edges[i]\n",
    "    print('For class: ',classes[i])\n",
    "    wr, acc, bleu = accuracy_printerXY(X_val_c[initial:terminal],y_val[initial:terminal],model)\n",
    "    wer_classwise.append(wr)\n",
    "    acc_classwise.append(acc)\n",
    "    bleu_classwise.append(bleu)\n",
    "    \n",
    "    initial = terminal\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "print(wer_classwise)\n",
    "print(acc_classwise)\n",
    "print(bleu_classwise)\n",
    "\n",
    "\n",
    " accuracy_printerXY(X_val_c,y_val,model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
